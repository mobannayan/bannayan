{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import GridSearchCV, train_test_split\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import classification_report, accuracy_score, confusion_matrix\n",
    "from sklearn.impute import SimpleImputer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set seed for reproducibility\n",
    "np.random.seed(42)\n",
    "\n",
    "# Step 1: Simulate features with different sample sizes\n",
    "sample_sizes = {\n",
    "    'soil_moisture': 1200,\n",
    "    'rainfall': 950,\n",
    "    'temperature': 1000,\n",
    "    'ndvi': 850\n",
    "}\n",
    "\n",
    "max_len = max(sample_sizes.values())\n",
    "# Create features with NaN padding\n",
    "features = {}\n",
    "for name, size in sample_sizes.items():\n",
    "    data = np.random.rand(size)\n",
    "    if size < max_len:\n",
    "        data = np.concatenate([data, [np.nan] * (max_len - size)])\n",
    "    features[name] = data\n",
    "\n",
    "df = pd.DataFrame(features)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We define a dictionary of synthetic \"features\" and their sample sizes.\n",
    "\n",
    "This simulates how much data we have for each environmental variable.\n",
    "\n",
    "    soil_moisture: 1200 samples → complete\n",
    "\n",
    "    rainfall: 950 samples → incomplete\n",
    "\n",
    "    temperature: 1000 samples → complete\n",
    "\n",
    "    ndvi: 850 samples → incomplete\n",
    "    max_len = max(sample_sizes.values())\n",
    "\n",
    "    Determine the maximum length (1200) so we can pad all shorter features with NaN up to this size.\n",
    "    For each feature:\n",
    "\n",
    "    Generate size number of random values between 0 and 1 using np.random.rand(size).\n",
    "\n",
    "    If the feature's sample size is less than max_len:\n",
    "\n",
    "        Append NaN values until its length becomes equal to the maximum.\n",
    "\n",
    "        This simulates missing data.\n",
    "\n",
    "    Add the padded array to the features dictionary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Features with missing data: ['rainfall', 'ndvi']\n"
     ]
    }
   ],
   "source": [
    "# Step 2: Mark features with <1000 valid entries as missing\n",
    "missing_features = [col for col in df.columns if df[col].count() < 1000]\n",
    "print(\"Features with missing data:\", missing_features)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 3: Impute missing values (e.g., with mean)\n",
    "imputer = SimpleImputer(strategy='mean')\n",
    "X = imputer.fit_transform(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "imputer = SimpleImputer(strategy='mean')\n",
    "\n",
    "    We create an imputer object that will replace missing values using the mean of each column (feature).\n",
    "\n",
    "    You can also change the strategy to:\n",
    "\n",
    "        'median' → for skewed data.\n",
    "\n",
    "        'most_frequent' → for categorical data.\n",
    "\n",
    "        'constant' → fill with a specific number.\n",
    "        X = imputer.fit_transform(df)\n",
    "\n",
    "    We apply the imputer to our DataFrame df, which contains missing values.\n",
    "\n",
    "    fit_transform() does two things:\n",
    "\n",
    "        Fit: Calculates the mean of each column (ignoring NaNs).\n",
    "\n",
    "        Transform: Replaces all NaN values in that column with the computed mean."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 4: Simulate a target variable (y) based on available features\n",
    "features_imputed = pd.DataFrame(X, columns=df.columns)\n",
    "\n",
    "score = (\n",
    "    0.3 * features_imputed['soil_moisture'] +\n",
    "    0.2 * features_imputed['rainfall'] -\n",
    "    0.4 * np.abs(features_imputed['temperature'] - 25) +\n",
    "    50 * features_imputed['ndvi']\n",
    ")\n",
    "# Use percentiles for balanced classes\n",
    "bins = np.percentile(score, [33, 66])\n",
    "\n",
    "y = np.digitize(score, bins=[50, 100])  # 0: low, 1: medium, 2: high"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "y = np.digitize(score, bins=[50, 100])\n",
    "\n",
    "    This line converts the continuous score into discrete classes (labels) for classification.\n",
    "\n",
    "    How np.digitize works:\n",
    "\n",
    "        bins=[50, 100] defines the thresholds.\n",
    "\n",
    "        Values <= 50 get class 0 (low yield class).\n",
    "\n",
    "        Values > 50 and <= 100 get class 1 (medium yield class).\n",
    "\n",
    "        Values > 100 get class 2 (high yield class)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Target class distribution: (array([0], dtype=int64), array([1200], dtype=int64))\n"
     ]
    }
   ],
   "source": [
    "# Check balance\n",
    "print(\"Target class distribution:\", np.unique(y, return_counts=True))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 5: Train/test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 6: Scale features\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=5, estimator=KNeighborsClassifier(),\n",
       "             param_grid={'metric': ['euclidean', 'manhattan'],\n",
       "                         'n_neighbors': [3, 5, 7],\n",
       "                         'weights': ['uniform', 'distance']})"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Step 7: Optimize KNN with cross-validation\n",
    "param_grid = {\n",
    "    'n_neighbors': [3, 5, 7],\n",
    "    'weights': ['uniform', 'distance'],\n",
    "    'metric': ['euclidean', 'manhattan']\n",
    "}\n",
    "\n",
    "knn = KNeighborsClassifier()\n",
    "grid = GridSearchCV(knn, param_grid, cv=5)\n",
    "grid.fit(X_train, y_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 8: Evaluate the model\n",
    "best_knn = grid.best_estimator_\n",
    "y_pred = best_knn.predict(X_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Best Parameters: {'metric': 'euclidean', 'n_neighbors': 3, 'weights': 'uniform'}\n",
      "Best Cross-Validation Score: 1.0\n",
      "Test Accuracy: 1.0\n",
      "\n",
      "Confusion Matrix:\n",
      " [[240]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00       240\n",
      "\n",
      "    accuracy                           1.00       240\n",
      "   macro avg       1.00      1.00      1.00       240\n",
      "weighted avg       1.00      1.00      1.00       240\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nBest Parameters:\", grid.best_params_)\n",
    "print(\"Best Cross-Validation Score:\", grid.best_score_)\n",
    "print(\"Test Accuracy:\", accuracy_score(y_test, y_pred))\n",
    "print(\"\\nConfusion Matrix:\\n\", confusion_matrix(y_test, y_pred))\n",
    "print(\"\\nClassification Report:\\n\", classification_report(y_test, y_pred))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Predictions for New Data:\n",
      "    soil_moisture  rainfall  temperature      ndvi  predicted_crop_class\n",
      "0       0.924073  0.425998    18.852331  0.725287                     0\n",
      "1       0.820324  0.967743    18.900618  0.360394                     0\n",
      "2       0.561864  0.657765    15.128981  0.087071                     0\n",
      "3       0.671406  0.564395    20.691739  0.202149                     0\n",
      "4       0.926308  0.932957    16.524296  0.545490                     0\n",
      "5       0.619168  0.586512     7.973820  0.207057                     0\n",
      "6       0.824807  0.526083    16.494659  0.543973                     0\n",
      "7       0.688811  0.759311     2.891078  0.231599                     0\n",
      "8       0.551936  0.712666    23.031435  0.848924                     0\n",
      "9       0.241838  0.193160    34.146182  0.397271                     0\n"
     ]
    }
   ],
   "source": [
    "# Step 9: Predict new unseen crop data (optional)\n",
    "test_data = pd.DataFrame({\n",
    "    'soil_moisture': np.random.rand(10),\n",
    "    'rainfall': np.random.rand(10),\n",
    "    'temperature': np.random.rand(10) * 40,\n",
    "    'ndvi': np.random.rand(10)\n",
    "})\n",
    "\n",
    "test_data_imputed = imputer.transform(test_data)\n",
    "test_data_scaled = scaler.transform(test_data_imputed)\n",
    "y_pred_missing = best_knn.predict(test_data_scaled)\n",
    "test_data['predicted_crop_class'] = y_pred_missing\n",
    "\n",
    "print(\"\\nPredictions for New Data:\\n\", test_data)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
